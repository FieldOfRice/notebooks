{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG45S3GHbqqq"
   },
   "source": [
    "## Python snippets\n",
    "\n",
    "Fetched from various web places or self build(former mobile screenshots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-24T12:26:07.604486Z",
     "iopub.status.busy": "2025-08-24T12:26:07.604221Z",
     "iopub.status.idle": "2025-08-24T12:26:07.748352Z",
     "shell.execute_reply": "2025-08-24T12:26:07.748050Z",
     "shell.execute_reply.started": "2025-08-24T12:26:07.604471Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kernel / Restart kernel ... / run this cell before any other cell, it's required by them\n",
    "import importlib.metadata as meta\n",
    "import pandas as pd\n",
    "\n",
    "# Note: There is no fixed mapping between package name and import name. Somehow you have to know it or someone told you.\n",
    "\n",
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        return None # Suppress traceback display\n",
    "\n",
    "def countdown(n):\n",
    "    while n > 0:\n",
    "        yield n\n",
    "        n -= 1\n",
    "\n",
    "def wrapped_enumerate(iterable, max_value):\n",
    "    for i, item in enumerate(iterable):\n",
    "        yield (i % max_value, item)        \n",
    "\n",
    "def waste_multiple_of_5s_with_feedback(waste_amount=6):\n",
    "    print(\"waiting \", end=\"\")\n",
    "    for _ in countdown(waste_amount):\n",
    "        print(_)\n",
    "        time.sleep(5)\n",
    "        print(\".\", end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "def display_table(table):\n",
    "    display((\n",
    "        pd\n",
    "        .DataFrame(table, columns=['Name', 'Version'])\n",
    "        .style\n",
    "        .set_properties(**{'text-align': 'left'})\n",
    "        .set_table_styles([{'selector': 'th', 'props': [('text-align', 'left')]}])\n",
    "        .hide(axis='index')\n",
    "    ))\n",
    "\n",
    "def find_package(search_words):\n",
    "    fault = False\n",
    "    installed_packages = sorted(\n",
    "        [(dist.metadata['Name'], dist.version) for dist in meta.distributions()], \n",
    "        key=lambda x: x[0].lower())\n",
    "    # display_table(installed_packages)\n",
    "    results = []\n",
    "    for word in search_words:\n",
    "        match = next(\n",
    "            ((pkg[0], pkg) for pkg in installed_packages if word.lower() in pkg[0].lower()),\n",
    "            None\n",
    "        )\n",
    "        if match:\n",
    "            results.append((word, match[1][1]))\n",
    "        else:\n",
    "            fault = True\n",
    "            print(\"A needed package is missing. Aborting cell execution\")\n",
    "            results.append((word, \"n/a\"))\n",
    "    if fault:\n",
    "        display_table(results)\n",
    "        raise StopExecution()\n",
    "    return results\n",
    "    \n",
    "# display_table(find_package([\"requests\", \"asyncio\", \"transformers\", \"numpy\", \"beautifulsoup4\", \"ddgs\", \"sentence-transformers\", \"scikit-learn\", \"pandas\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fetch only the code cells from *.ipynb and print them\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import nbformat\n",
    "import os\n",
    "\n",
    "notebook_files = [f for f in os.listdir('.') if f.endswith('.ipynb')]\n",
    "for nb_file in notebook_files:\n",
    "    with open(nb_file, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "    code_cells = [cell['source'] for cell in notebook['cells'] if cell['cell_type'] == 'code']\n",
    "    for cell_code in code_cells:\n",
    "        print(cell_code)\n",
    "    print(f\"\\n===== End of {nb_file} =====\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Open webbrowser with file automatically\n",
    "import os\n",
    "import webbrowser\n",
    "\n",
    "try:\n",
    "    # chrome_path = '/usr/bin/google-chrome %s'\n",
    "    # webbrowser.register('chrome', None, webbrowser.BackgroundBrowser(chrome_path))\n",
    "    chrome = webbrowser.get('chrome')\n",
    "    success = chrome.open(f\"file:///{os.path.abspath(output_file)}\")\n",
    "    if not success:\n",
    "        print(\"Failed to open the browser.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error opening browser: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_package([\"ddgs\"])\n",
    "# kernel reset might be neccessary, because of caches, ...\n",
    "import logging\n",
    "import http.client as http_client\n",
    "http_client.HTTPConnection.debuglevel = 1\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "requests_log = logging.getLogger(\"requests.packages.urllib3\")\n",
    "requests_log.setLevel(logging.DEBUG)\n",
    "requests_log.propagate = True\n",
    "# usage example:\n",
    "from ddgs import DDGS\n",
    "ddgs = DDGS()\n",
    "ddgs.text(\"Gimme me weather forecast pages\", max_results=5)\n",
    "# ... DDGS aggregates search results from multiple engines, not just DuckDuckGo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolated notebook related environment using uv, but be reminded that now each notebook needs it's own folder\n",
    "# mkdir folder; cd folder\n",
    "!uv init\n",
    "!uv add --dev ipykernel\n",
    "!uv run ipython kernel install --user --env VIRTUAL_ENV=$(uv env) --name=notebook_task_1 --display-name=\"notebook (task 1)\"\n",
    "!uv add pandas ddgs matplotlib\n",
    "# Go to Kernel > Change Kernel in JupyterLab UI.\n",
    "# Select the newly created kernel named \"notebook (task 1)\"\n",
    "# \n",
    "# working inside this notebooks with this kernel has an isolated environment using only the above added packages\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty print the firefox bookmarks json export, use vi and daB do delete from { to } including the two braces.\n",
    "import json\n",
    "filename = \"data/firefox_bookmarks.json\"\n",
    "try:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        bookmarks = json.load(f)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\"JSON syntax error \", e)\n",
    "pretty_bookmarks = json.dumps(bookmarks, indent=4, ensure_ascii=False)\n",
    "print(pretty_bookmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this old snippet uses the GPU to run the given ollama model, try/catch still missing\n",
    "from ollama import chat\n",
    "\n",
    "OLLAMA_MODEL = \"qwen2.5-coder:32b-instruct\"\n",
    "\n",
    "def create_messages():\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"gimme a python code example using DDGS() call to execute a query and return 10 urls \"\n",
    "                \"checking them with cosine_similarity to proove they are related to the given query \"\n",
    "                \"and present a summarize of each.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def ask_ai(messages, max_retries=3):\n",
    "    response = chat(model=OLLAMA_MODEL, messages=messages, options={\"num_gpu\": 132}) # force GPU usage\n",
    "    return response.message.content\n",
    "\n",
    "print(ask_ai(create_messages()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "def create_messages(user_task, input_data):\n",
    "    prompt = f\"\"\"\n",
    "    You are an automation assistant trained in Python development and scripting.\n",
    "    Your job is to take the following user input and perform a highly specific automated transformation.\n",
    "    ### Here's the task:\n",
    "    {user_task}\n",
    "    ### Here's the input data (if any):\n",
    "    {input_data}\n",
    "    ### Your job is to:\n",
    "    - Write complete Python code for automating the task.\n",
    "    - Include all necessary libraries and handle exceptions.\n",
    "    - Optimize for readability and reuse.\n",
    "    - Return only runnable Python code (no explanation).\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Python automation engineer.\"},\n",
    "        {\"role\": \"user\",   \"content\": prompt }\n",
    "    ]\n",
    "\n",
    "def doit():\n",
    "    OLLAMA_MODEL = \"qwen2.5-coder:32b-instruct\"\n",
    "    user_task  = \"Fix malformed CSV where some rows have more or fewer columns than the header. Align data properly. Drop incomplete rows.\"\n",
    "    input_data = \"data/broken.csv\"\n",
    "    response = chat(model=OLLAMA_MODEL, messages=create_messages(user_task, input_data), options={\"num_gpu\": 132, \"num_ctx\": 32768}) # force GPU usage\n",
    "    print(response.message.content)\n",
    "\n",
    "doit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... the same but with gradio GUI\n",
    "find_package([\"gradio\",\"IProgress\"])\n",
    "import time\n",
    "import gradio as gr\n",
    "from IPython.display import display, HTML\n",
    "from ollama import chat\n",
    "\n",
    "def create_messages(user_task, input_data):\n",
    "    prompt = f\"\"\"\n",
    "    You are an automation assistant trained in Python development and scripting.\n",
    "    Your job is to take the following user input and perform a highly specific automated transformation.\n",
    "    ### Here's the task:\n",
    "    {user_task}\n",
    "    ### Here's the input data (if any):\n",
    "    {input_data}\n",
    "    ### Your job is to:\n",
    "    - Write complete Python code for automating the task.\n",
    "    - Include all necessary libraries and handle exceptions.\n",
    "    - Optimize for readability and reuse.\n",
    "    - Return only runnable Python code (no explanation).\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Python automation engineer.\"},\n",
    "        {\"role\": \"user\",   \"content\": prompt }\n",
    "    ]\n",
    "\n",
    "def generate_code(task, data):\n",
    "    response = chat(model=OLLAMA_MODEL, messages=create_messages(user_task, input_data), options={\"num_gpu\": 132, \"num_ctx\": 32768}) # force GPU usage\n",
    "    return response.message.content\n",
    "\n",
    "# TODO: change it to use block / finish_btn like below ...\n",
    "def interface():\n",
    "    OLLAMA_MODEL = \"qwen2.5-coder:32b-instruct\"\n",
    "    user_task  = \"Fix malformed CSV where some rows have more or fewer columns than the header. Align data properly. Drop incomplete rows.\"\n",
    "    input_data = \"data/broken.csv\"\n",
    "    user_task_input  = gr.Textbox(label=\"Task Description\",   lines=10, placeholder=user_task)\n",
    "    input_data_input = gr.Textbox(label=\"Input Data or Path\", lines=3,  placeholder=input_data)\n",
    "    iface = gr.Interface(\n",
    "        fn=generate_code,\n",
    "        inputs=[user_task_input, input_data_input],\n",
    "        outputs=gr.Textbox(label=\"Generated Python Code\", lines=25),\n",
    "        title=\"Auto Developer (120 second timeout)\",\n",
    "        description=\"Describe the task and paste input - get working Python code.\",\n",
    "        css=\"footer {display:none !important;}\"\n",
    "    )\n",
    "    iface.launch(server_name=\"0.0.0.0\", server_port=10000) # run it locally is preferred\n",
    "    display(HTML(f'<a href=\"http://localhost:17779\" target=\"_blank\" style=\"font-size:20px;\"> --- Click me followed by a click at Submit ---</a>'))\n",
    "    waste_multiple_of_5s_with_feedback(24)\n",
    "    iface.close() # keeps the port reusable\n",
    "\n",
    "interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is one response from the above requests... it just works... this time...\n",
    "import pandas as pd\n",
    "def fix_malformed_csv(file_path):\n",
    "    try:\n",
    "        # Read the CSV file, assuming no header is provided to avoid misalignment\n",
    "        df = pd.read_csv(file_path, header=None)\n",
    "        # Determine the expected number of columns based on the longest row\n",
    "        expected_columns = df.shape[1]\n",
    "        # Drop rows that do not match the expected number of columns\n",
    "        cleaned_df = df[df.isnull().sum(axis=1) == 0]\n",
    "        # Assuming the first row contains the header, separate it and set as the header\n",
    "        if not cleaned_df.empty:\n",
    "            headers = cleaned_df.iloc[0].tolist()\n",
    "            cleaned_df = cleaned_df[1:].reset_index(drop=True)\n",
    "            cleaned_df.columns = headers\n",
    "        # Save the cleaned DataFrame back to CSV\n",
    "        output_file_path = 'data/cleaned.csv'\n",
    "        cleaned_df.to_csv(output_file_path, index=False)\n",
    "        print(f\"Cleaned data saved to {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "# Example usage\n",
    "fix_malformed_csv('data/broken.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer visual displayed\n",
    "find_package([\"gradio\",\"transformers\", \"sentencepiece\", \"protobuf\"]) # gradio 5.43.1\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TQDM\"] = \"true\" # no tqdm console output of transformer downloads\n",
    "import random\n",
    "import time\n",
    "import threading\n",
    "import gradio as gr\n",
    "from IPython.display import HTML\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def colorize(text, model_name):\n",
    "    colors = [ # 32 preselected visible colors\n",
    "        \"#FF5733\", \"#33FF57\", \"#3357FF\", \"#FF33A1\", \"#A133FF\", \"#33FFF6\",\n",
    "        \"#FFC733\", \"#FF8633\", \"#33FF8A\", \"#338AFF\", \"#FF3380\", \"#8033FF\",\n",
    "        \"#33FFD1\", \"#FFE933\", \"#FF6E33\", \"#33FFB2\", \"#336FFF\", \"#FF3377\",\n",
    "        \"#9B33FF\", \"#33FFE3\", \"#FF9A33\", \"#33FFA1\", \"#335CFF\", \"#FF33D1\",\n",
    "        \"#A633FF\", \"#33FFD7\", \"#FFE533\", \"#FF5733\", \"#33FF74\", \"#3380FF\",\n",
    "        \"#FF33B2\", \"#8033FF\" ]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    encoding = tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=False)\n",
    "    offsets = encoding['offset_mapping']\n",
    "    ids = encoding['input_ids']\n",
    "    tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "    colors_available = len(colors)\n",
    "    missing_colors = len(tokens)-colors_available\n",
    "    if missing_colors>0:\n",
    "        print(f\"Need {missing_colors} more colors. Now reusing existing colors.\")\n",
    "    all_tokens = \"\"\n",
    "    all_ids    = \"\"\n",
    "    for index, (id_val, token, (start, end)) in wrapped_enumerate(zip(ids, tokens, offsets), colors_available):\n",
    "        all_tokens += f\"<td>&nbsp;<span style='color: {colors[index]};'>{text[start:end]}</span></td>\"\n",
    "        all_ids    += f\"<td style='text-align: center;'>{id_val:d}&nbsp</td>\"\n",
    "    # one table for each transformer to achive improved readability (only a tiny amount of space after each token)\n",
    "    result = (f\"<table class='no-space'><tr style='font-size: 20px;'>{all_tokens}<td style='text-align: center; color: red;'>{model_name}</td></tr>\"\n",
    "              f\"<tr>{all_ids}<td style='text-align: center; color: green;'>token count {len(tokens)}</td></tr></table>\")\n",
    "    return result\n",
    "    \n",
    "def colorize_text_with_transformer(text, model_name, all_transformers, progress=gr.Progress()):\n",
    "    if not model_name:\n",
    "        model_name = all_transformers[0]\n",
    "    if model_name != all_transformers[-1]:\n",
    "        result = colorize(text, model_name)\n",
    "    else:\n",
    "        progress(0, desc=\"Starting...\")\n",
    "        total = len(all_transformers[:-2])\n",
    "        result = \"\"\n",
    "        for i, model_name in enumerate(all_transformers[:-2]): # the M$ transformer prefers to be single\n",
    "            progress(i/total, desc=f\"Processing {model_name}\")\n",
    "            result += colorize(text, model_name)\n",
    "        progress(1.0, desc=\"Done!\")\n",
    "    return result, gr.update(interactive=True) # enable finish button\n",
    "\n",
    "def interface():\n",
    "    transformer_models = [ \"bert-base-uncased\", \"bert-base-cased\", \"roberta-base\", \"distilbert-base-uncased\", \"xlm-roberta-base\",\n",
    "                           \"albert-base-v2\", \"gpt2\", \"facebook/bart-base\", \"t5-small\", \"openai-gpt\",\n",
    "                           \"google/electra-base-discriminator\", \"allenai/longformer-base-4096\",\n",
    "                           \"google/pegasus-large\", \"microsoft/deberta-v3-base\", \"Everything but the last\" ]\n",
    "    css = \"\"\"\n",
    "    footer { display: none !important; } /* hide gradio marketing footer */\n",
    "    #customtextbox textarea { font-size: 24px !important; } /* font enlargement */\n",
    "    #customdropdown input { font-size: 20px; } /* font enlargement dropdown selected text */\n",
    "    ul { font-size: 20px; } /* font enlargement dropdown listbox texts */\n",
    "    table.no-space { border-collapse: collapse; border: 1px solid black; } /* dense table */\n",
    "    table.no-space th, table.no-space td { border: 1px solid black; padding: 0; } /* tiny cell padding */\n",
    "    .progress-text { display: none !important; } /* hide seconds counter */\n",
    "    .progress-level-inner { font-size: 20px !important; } /* enlarge progress description */\n",
    "    \"\"\"\n",
    "    text = \"Many words maps to one token, but not always: unaffordable. Characters might be grouped: 1234567890.\"\n",
    "    SERVER_IP=\"0.0.0.0\"\n",
    "    SERVER_PORT=10000\n",
    "    demo = gr.Blocks(css=css, title=\"Colorful Token Visualization of Text with Transformer Selection\")\n",
    "    def close_app():\n",
    "        print(\"Finish was clicked. Closing the App...\")\n",
    "        demo.close() # does not always get executed\n",
    "        return\n",
    "    with demo:\n",
    "        with gr.Row():\n",
    "            input_text = gr.Textbox(label=\"Input Text (english letters, digits, and punctuation)\", value=text, elem_id=\"customtextbox\")\n",
    "            model_dropdown = gr.Dropdown(label=\"Choose Transformer\", value=transformer_models[0], choices=transformer_models, elem_id=\"customdropdown\")\n",
    "        submit_btn = gr.Button(\"Submit\", interactive=True)\n",
    "        finish_btn = gr.Button(\"Finish\", interactive=False) # Submit button should be clicked at least once ...\n",
    "        output_html = gr.HTML(label=\"Output Text\", value=\"<b>Click at Submit...</b>\")\n",
    "        submit_btn.click(lambda text, model_name: colorize_text_with_transformer(text, model_name, transformer_models), \n",
    "                         inputs=[input_text, model_dropdown], \n",
    "                         outputs=[output_html, finish_btn])\n",
    "        finish_btn.click(fn=close_app, inputs=[])\n",
    "        demo.launch(server_name=SERVER_IP, server_port=SERVER_PORT, prevent_thread_lock=True)\n",
    "\n",
    "display(HTML((f'<a href=\"http://localhost:17779\" target=\"_blank\" style=\"font-size:20px;\"> --- Click me followed by a click at Submit ---</a><br>'\n",
    "              f'<span style=\"font-size:20px;\">When you are done, click at the button Finish.</span><br><hr>')))\n",
    "interface()\n",
    "# execution continues, but sometimes the app doesn't close properly, so a kernel restart is required to run it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_package([\"ddgs\", \"python-whois\"])\n",
    "import requests\n",
    "import whois\n",
    "from http import HTTPStatus\n",
    "\n",
    "def is_domain_active(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == HTTPStatus.OK:\n",
    "            return True\n",
    "        if response.status_code == HTTPStatus.GONE:\n",
    "            return False\n",
    "        print(f\"Http status code of {url} is {response.status_code}\")            \n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def is_domain_registered(domain_name):\n",
    "    try:\n",
    "        domain_info = whois.whois(domain_name)\n",
    "        # If domain_info has expiration_date or registrar info, domain likely registered\n",
    "        if domain_info.domain_name:\n",
    "            # print(domain_info)\n",
    "            expiration = domain_info.expiration_date\n",
    "            if isinstance(expiration, list):\n",
    "                expiration = expiration[0]\n",
    "            print(f\"Expiration date of {domain_name} is {expiration}\")            \n",
    "            return True\n",
    "    except Exception as e:\n",
    "        # Could not retrieve WHOIS info, likely unregistered or blocked\n",
    "        print(e)\n",
    "        return False\n",
    "    return False\n",
    "\n",
    "def bool_to_human(value):\n",
    "    return (\"no\", \"yes\")[value]\n",
    "    \n",
    "# url = \"http://example.com\"\n",
    "# url = \"http://google.com\"\n",
    "url = \"http://www1.pybrain.org/?tm=1&subid4=1755681068.0192320000\"\n",
    "domain = url.split(\"//\")[-1].split(\"/\")[0]\n",
    "active = is_domain_active(url)\n",
    "registered = is_domain_registered(domain)\n",
    "print(f\"Domain {domain} active {bool_to_human(active)} registered {bool_to_human(registered)}\")\n",
    "if registered and not active:\n",
    "    print(\"Domain may be parked or down.\")\n",
    "elif not registered:\n",
    "    print(\"Domain is likely available for registration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request web page with session, user agent and reliable timeout using requests - needs about 3 seconds to run\n",
    "find_package([\"beautifulsoup4\"])\n",
    "import requests\n",
    "import random\n",
    "import inspect\n",
    "import signal\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_0) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:117.0) Gecko/20100101 Firefox/117.0\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Mobile Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPad; CPU OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Edg/139.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Linux; Android 12; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Mobile Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:117.0) Gecko/20100101 Firefox/117.0\"\n",
    "    # Add more user agents\n",
    "]\n",
    "headers = {\n",
    "    'User-Agent': random.choice(USER_AGENTS),\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Referer': 'https://www.google.com/'\n",
    "}\n",
    "\n",
    "def print_full_object(obj):\n",
    "    print(\"All attributes and methods:\")\n",
    "    for name, value in inspect.getmembers(obj):\n",
    "        print(f\"{name}: {value}\")\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutException(\"Request timed out\")\n",
    "\n",
    "def display_headers(headers):\n",
    "    max_key_len = max(len(k) for k in headers.keys())\n",
    "    print(f\"{'Header':<{max_key_len}} | Value\")\n",
    "    print(f\"{'-' * max_key_len}-|----------------\")\n",
    "    for key, value in headers.items():\n",
    "        print(f\"{key:<{max_key_len}} | {value}\")\n",
    "    \n",
    "timeout_seconds = 3\n",
    "signal.signal(signal.SIGALRM, handler)\n",
    "print(headers)\n",
    "try:\n",
    "    signal.alarm(timeout_seconds)\n",
    "    response = requests.Session().get('https://example.com', headers=headers, timeout=timeout_seconds)\n",
    "    signal.alarm(0)  # disable alarm on success\n",
    "    display_headers(response.headers)\n",
    "    soup     = BeautifulSoup(response.content, 'html.parser')\n",
    "    response = soup.prettify()\n",
    "    print(f\"response length is {len(response)} first 350 are:\\n{response[:350]}\")\n",
    "except TimeoutException:\n",
    "    print(\"Request timed out. Skipping response processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request web page with session, user agent and reliable timeout using httpx instead of requests\n",
    "find_package([\"beautifulsoup4\",\"pandas\"])\n",
    "import httpx, random, signal\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_0) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:117.0) Gecko/20100101 Firefox/117.0\",\n",
    "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Linux; Android 13; Pixel 7 Pro) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Mobile Safari/537.36\",\n",
    "    \"Mozilla/5.0 (iPad; CPU OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1\",\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Edg/139.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Linux; Android 12; SM-G991B) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Mobile Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:117.0) Gecko/20100101 Firefox/117.0\"\n",
    "    # Add more user agents\n",
    "]\n",
    "headers = {\n",
    "    'User-Agent': random.choice(USER_AGENTS),\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Referer': 'https://www.google.com/'\n",
    "}\n",
    "\n",
    "def display_headers(headers):\n",
    "    # headers_df = pd.DataFrame(response.headers.items(), columns=['Header', 'Value'])\n",
    "    display((\n",
    "        pd\n",
    "        .DataFrame(headers.items(), columns=['Header', 'Value'])\n",
    "        .style\n",
    "        .set_properties(**{'text-align': 'left'})\n",
    "        .set_table_styles([{'selector': 'th', 'props': [('text-align', 'left')]}])\n",
    "        .hide(axis='index')\n",
    "    ))\n",
    "    # for header, value in headers.items():\n",
    "    #    print(f\"{header}: {value}\")\n",
    "\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutException(\"Request timed out\")\n",
    "\n",
    "def main():\n",
    "    timeout_seconds = 3\n",
    "    signal.signal(signal.SIGALRM, handler)\n",
    "    print(headers)\n",
    "    try:\n",
    "        signal.alarm(timeout_seconds)\n",
    "        with httpx.Client() as client:\n",
    "            response = client.get('https://example.com')\n",
    "            signal.alarm(0)  # disable alarm on success\n",
    "            display_headers(response.headers)\n",
    "            soup     = BeautifulSoup(response.content, 'html.parser')\n",
    "            response = soup.prettify()\n",
    "            print(f\"response length is {len(response)} first 350 are:\\n{response[:350]}\")\n",
    "    except TimeoutException:\n",
    "        print(\"Request timed out. Skipping response processing.\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-20T12:25:44.887166Z",
     "iopub.status.busy": "2025-08-20T12:25:44.886851Z",
     "iopub.status.idle": "2025-08-20T12:25:44.889356Z",
     "shell.execute_reply": "2025-08-20T12:25:44.889095Z",
     "shell.execute_reply.started": "2025-08-20T12:25:44.887151Z"
    }
   },
   "source": [
    "### Here we have a styled text example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#383838; padding:16px;font-size: 24px;font-family: 'Alte Schwabacher';\"> ⚠️ <b>Use the AI bot: Ask how to format numbers of type float using Python f-strings</b></p>\n",
    "<!-- Kabinett Fraktur / Olde English / Koch Fette Deutsche Schrift UNZ1A / Walbaum\\-Fraktur UNZ1 / Alte Schwabacher -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T18:43:45.983968Z",
     "iopub.status.busy": "2025-08-21T18:43:45.983664Z",
     "iopub.status.idle": "2025-08-21T18:43:45.985866Z",
     "shell.execute_reply": "2025-08-21T18:43:45.985496Z",
     "shell.execute_reply.started": "2025-08-21T18:43:45.983957Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import shutil\n",
    "shutil.rmtree(matplotlib.get_cachedir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_package([\"matplotlib\"])\n",
    "import matplotlib.font_manager\n",
    "# List all font names available in matplotlib (usually system fonts)\n",
    "fonts = sorted(set(f.name for f in matplotlib.font_manager.fontManager.ttflist))\n",
    "display(fonts)\n",
    "# the font must be available at the host where the browser is running and NOT where jupyter is running.\n",
    "# cp /path/to/OldEnglishFont.ttf ~/.local/share/fonts/old_english/\n",
    "# fc-cache -f -v\n",
    "# fc-list | grep -i \"english\"\n",
    "# import matplotlib\n",
    "# import shutil\n",
    "# shutil.rmtree(matplotlib.get_cachedir())\n",
    "# restart kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "def do_something(seconds):\n",
    "    print(f'Sleeping {seconds} second(s)...\\n', end='') # cheap multiprocessing output handling\n",
    "    time.sleep(seconds)\n",
    "    return f'Done Sleeping...{seconds}'\n",
    "\n",
    "start = time.perf_counter()\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = executor.map(do_something, list(reversed(range(1, 6))))\n",
    "    if False: # change the word False at the left to True to see what was done inside do_something ...\n",
    "        for result in results:\n",
    "            print(result)\n",
    "finish = time.perf_counter()\n",
    "print(f'Finished in {round(finish-start, 2)} second(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the output is part of the result, this prevents mixed output\n",
    "# threads   --> use them if I/O bound\n",
    "# processes --> use them if CPU bound\n",
    "# brilliant.org/CMS\n",
    "import concurrent.futures\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def now():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"[%Y%m%d %H%M%S] \")\n",
    "\n",
    "def do_something(seconds):\n",
    "    result = []\n",
    "    result.append(f'{now()}Sleeping {seconds} second(s)...')\n",
    "    time.sleep(seconds)\n",
    "    result.append(f'{now()}Done Sleeping...{seconds}')\n",
    "    return result\n",
    "\n",
    "def main():\n",
    "    start = time.perf_counter()\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "        results = executor.map(do_something, list(reversed(range(1, 11)))) # [10..1]\n",
    "        # execution continues here after all results have arrived\n",
    "        if False: # change the word False at the left to True to see what happens inside do_something ...\n",
    "            for result in results:\n",
    "                for item in result:\n",
    "                    print(item)\n",
    "    finish = time.perf_counter()\n",
    "    print(f'Finished in {round(finish-start, 2)} second(s)')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity example\n",
    "find_package([\"scikit-learn\",\"pandas\"])\n",
    "import pandas as pd\n",
    "import re\n",
    "from IPython.display import HTML, display\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "twitter = \"\"\"\n",
    "Twitter is an online social media and social networking service owned and operated by American company X Corp.,\n",
    "the legal successor of Twitter, Inc. Twitter users outside the United States are legally served by the Ireland-based\n",
    "Twitter International Unlimited Company, which makes these users subject to Irish and European Union data protection laws.\n",
    "On Twitter users post texts, photos and videos known as 'tweets'. Registered users can tweet, like, 'retweet' tweets,\n",
    "and direct message (DM) other registered users, while unregistered users only have the ability to view public tweets.\n",
    "Users interact with Twitter through browser or mobile frontend software, or programmatically via its APIs.\n",
    "\"\"\"\n",
    "facebook = \"\"\"\n",
    "Facebook is an online social media and social networking service owned by American technology giant Meta Platforms.\n",
    "Created in 2004 by Mark Zuckerberg with fellow Harvard College students and roommates Eduardo Saverin, Andrew McCollum,\n",
    "Dustin Moskovitz, and Chris Hughes, its name derives from the face book directories often given to American university students.\n",
    "Membership was initially limited to only Harvard students, gradually expanding to other North American universities and,\n",
    "since 2006, anyone over 13 years old. As of December 2022, Facebook claimed 2.96 billion monthly active users, and ranked third\n",
    "worldwide among the most visited websites. It was the most downloaded mobile app of the 2010s. Facebook can be accessed from devices\n",
    "with Internet connectivity, such as personal computers, tablets and smartphones. After registering, users can create a profile\n",
    "revealing information about themselves. They can post text, photos and multimedia which are shared with any other users who have\n",
    "agreed to be their friend' or, with different privacy settings, publicly. Users can also communicate directly with each other with\n",
    "Messenger, join common-interest groups, and receive notifications on the activities of their Facebook friends and the pages they follow.\n",
    "\"\"\"\n",
    "tiktok = \"\"\"\n",
    "TikTok, and its Chinese counterpart Douyin (Chinese: 抖音; pinyin: Dǒuyīn), is a short-form video hosting service owned by ByteDance.\n",
    "It hosts user-submitted videos, which can range in duration from 3 seconds to 10 minutes. Since their launches, TikTok and Douyin have\n",
    "gained global popularity.[6][7] In October 2020, TikTok surpassed 2 billion mobile downloads worldwide. Morning Consult named TikTok the\n",
    "third-fastest growing brand of 2020, after Zoom and Peacock. Cloudflare ranked TikTok the most popular website of 2021,\n",
    "surpassing google.com.\n",
    "\"\"\"\n",
    "instagram = \"\"\"\n",
    "Instagram is a photo and video sharing social networking service owned by American company Meta Platforms. The app allows users to\n",
    "upload media that can be edited with filters and organized by hashtags and geographical tagging. Posts can be shared publicly or\n",
    "with preapproved followers. Users can browse other users' content by tag and location, view trending content, like photos, and follow\n",
    "other users to add their content to a personal feed. Instagram was originally distinguished by allowing content to be framed only in a\n",
    "square (1:1) aspect ratio of 640 pixels to match the display width of the iPhone at the time. In 2015, this restriction was eased with\n",
    "an increase to 1080 pixels. It also added messaging features, the ability to include multiple images or videos in a single post, and a\n",
    "Stories feature—similar to its main competitor Snapchat—which allowed users to post their content to a sequential feed, with each post\n",
    "accessible to others for 24 hours. As of January 2019, Stories is used by 500 million people daily.\n",
    "\"\"\"\n",
    "\n",
    "def display_no_size_info(df, max_cols=20):\n",
    "    \"\"\"\n",
    "    Display a pandas DataFrame in Jupyter/IPython without showing the size information footer.\n",
    "\n",
    "    This function converts the DataFrame to HTML, removes the default pandas footer \n",
    "    that displays the number of rows and columns (e.g., \"[4 rows x 302 columns]\"), \n",
    "    and then renders the cleaned HTML for display.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The DataFrame to display without the size summary.\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - This function is intended for use in Jupyter notebooks or IPython environments\n",
    "      where pandas DataFrames are rendered as HTML tables.\n",
    "    - The method relies on regex removal of the HTML footer and may need adjustment \n",
    "      if pandas changes its HTML structure in future versions.\n",
    "    \"\"\"\n",
    "    # Get the HTML representation of the DataFrame\n",
    "    html = df.to_html(max_cols=max_cols)\n",
    "    # The footer div usually starts with <div class=\"dataframe_info\"> or similar; find and remove it.\n",
    "    cleaned_html = re.sub(r'<div class=\"dataframe_info\">.*?</div>', '', html, flags=re.DOTALL)\n",
    "    display(HTML(cleaned_html))\n",
    "\n",
    "documents        = [twitter, facebook, tiktok, instagram]\n",
    "count_vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "count_vectorizer = CountVectorizer() # we want all words ...\n",
    "sparse_matrix    = count_vectorizer.fit_transform(documents)\n",
    "doc_term_matrix  = sparse_matrix.todense()\n",
    "df = pd.DataFrame(\n",
    "   doc_term_matrix,\n",
    "   columns=count_vectorizer.get_feature_names_out(),\n",
    "   index=[\"twitter\", \"facebook\", \"tiktok\", \"instagram\"],\n",
    ")\n",
    "print(f\"That are {df.shape[1]} columns\")\n",
    "display_no_size_info(df)                          # formatted output\n",
    "# pd.set_option('display.width', 2000)            # Disable line wrapping width limit\n",
    "# pd.set_option('display.max_columns', 20)        # Display 20 columns\n",
    "# print(df.to_string(max_rows=None, max_cols=20)) # pure textual output without size line\n",
    "\n",
    "# compute cosine similarity\n",
    "similarity = cosine_similarity(df, df)\n",
    "\n",
    "# wrap in DataFrame with labels\n",
    "sim_df = pd.DataFrame(\n",
    "    similarity,\n",
    "    index=df.index,  # row labels\n",
    "    columns=df.index # column labels\n",
    ")\n",
    "display(sim_df)\n",
    "\n",
    "# result with stopwords and 302 columns                result with stopwords removed and 250 columns\n",
    "#             twitter  facebook    tiktok  instagram              twitter  facebook    tiktok  instagram\n",
    "# twitter    1.000000  0.498128  0.232696   0.493960   twitter    1.000000  0.282005  0.035266   0.335484\n",
    "# facebook   0.498128  1.000000  0.349222   0.603528   facebook   0.282005  1.000000  0.053773   0.262330\n",
    "# tiktok     0.232696  0.349222  1.000000   0.304792   tiktok     0.035266  0.053773  1.000000   0.035266\n",
    "# instagram  0.493960  0.603528  0.304792   1.000000   instagram  0.335484  0.262330  0.035266   1.000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just debug the tcp communication of a docker container and do not modify it\n",
    "- https://github.com/byF/docker-tcpflow\n",
    "- https://github.com/byF/docker-tcpflow/blob/master/Dockerfile\n",
    "- docker run --net=\"container:n8n-ollama\" byfcz/tcpflow -p -c\n",
    "- see also ~/docker/tools providing lsof, netstat, ss and vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CPU temperature displayed at the top/middle \n",
    "import tkinter as tk\n",
    "import signal\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "# cat /sys/class/hwmon/hwmon2/temp1_label # at this host it is Tctl which is CPU\n",
    "TEMP_FILE = \"/sys/class/hwmon/hwmon2/temp1_input\"\n",
    "TEMP_THRESHOLD = 65\n",
    "UPDATE_INTERVAL_MS = 250\n",
    "UPDATE_SKIP_COUNT = 4\n",
    "\n",
    "# could be class-based, but isn't yet\n",
    "\n",
    "counter: int = 0\n",
    "temp: Optional[float] = None\n",
    "\n",
    "def read_cpu_temperature() -> Optional[float]:\n",
    "    \"\"\"\n",
    "    Read CPU temperature in Celsius from the system sensor file.\n",
    "\n",
    "    Returns:\n",
    "        float: Temperature in Celsius, or None if reading failed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(TEMP_FILE, \"r\") as f:\n",
    "            temp_str = f.read().strip()\n",
    "            temp_c   = int(temp_str) / 1000 # value is milli celsius\n",
    "            return temp_c\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def update_temperature() -> None:\n",
    "    \"\"\"\n",
    "    Update temperature label text and color periodically.\n",
    "    \"\"\"\n",
    "    global counter, temp\n",
    "    if counter == 0:\n",
    "        temp = read_cpu_temperature()\n",
    "        if temp is not None:\n",
    "            text = f\"{int(temp)}°C\"\n",
    "            if temp < TEMP_THRESHOLD:\n",
    "                label.config(text=text, fg=\"green\")\n",
    "            else:\n",
    "                label.config(text=text, fg=\"red\")\n",
    "        else:\n",
    "            label.config(text=\"--°C\", fg=\"yellow\")\n",
    "    counter = (counter + 1) % UPDATE_SKIP_COUNT\n",
    "    root.after(UPDATE_INTERVAL_MS, update_temperature)\n",
    "\n",
    "def center_window() -> None:\n",
    "    \"\"\"\n",
    "    Center the window horizontally near the top of the screen.\n",
    "    \"\"\"\n",
    "    root.update_idletasks()\n",
    "    width  = root.winfo_width()\n",
    "    height = root.winfo_height()\n",
    "    screen_width = root.winfo_screenwidth()\n",
    "    x = (screen_width // 2) - (width // 2)\n",
    "    y = 30\n",
    "    root.geometry(f\"{width}x{height}+{x}+{y}\")\n",
    "\n",
    "def signal_handler(sig: int, frame: Optional[object]) -> None:\n",
    "    \"\"\"\n",
    "    Cleanly exit the application on signal interrupt.\n",
    "    \"\"\"\n",
    "    root.destroy()\n",
    "    sys.exit(0)\n",
    "\n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "root = tk.Tk()\n",
    "root.title(\"CPU Temperature\")\n",
    "root.overrideredirect(True)\n",
    "root.attributes(\"-topmost\", True)\n",
    "root.attributes(\"-alpha\", 0.7)\n",
    "label = tk.Label(root, text=\"\", font=(\"Helvetica\", 24), bg=\"black\")\n",
    "label.pack()\n",
    "update_temperature()\n",
    "center_window()u\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start python script when user logs into the system\n",
    "\n",
    "create ~/.config/systemd/user/python-script.service\n",
    "```\n",
    "[Unit]\n",
    "Description=Run Python Script on user login\n",
    "\n",
    "[Service]\n",
    "ExecStart=/usr/bin/bash -c 'sleep 20 && /usr/bin/python3 /home/username/code.py'\n",
    "Restart=on-failure\n",
    "\n",
    "[Install]\n",
    "WantedBy=default.target\n",
    "```\n",
    "commands to activate that:\n",
    "```\n",
    "systemctl --user daemon-reload\n",
    "systemctl --user enable python-script.service\n",
    "systemctl --user start python-script.service\n",
    "```\n",
    "command to check status:\n",
    "```\n",
    "systemctl --user status python-script.service\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7anm-tFjb0d2"
   },
   "source": [
    "### pyinstrument aka flmagraph profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqONtE6Nbu4D"
   },
   "outputs": [],
   "source": [
    "# pyinstrument my_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10287,
     "status": "ok",
     "timestamp": 1753008686584,
     "user": {
      "displayName": "Its Me",
      "userId": "05939231591080462335"
     },
     "user_tz": -120
    },
    "id": "P2xfvQidb9Lo",
    "outputId": "2fba10a6-9d84-417b-918f-bfaa0705f9c3"
   },
   "outputs": [],
   "source": [
    "!pip install pyinstrument\n",
    "import time\n",
    "from pyinstrument import Profiler\n",
    "profiler = Profiler()\n",
    "profiler.start()\n",
    "time.sleep(0.5)\n",
    "profiler.stop()\n",
    "print(profiler.output_text(unicode=True, color=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRxlUvBTdlfY"
   },
   "source": [
    "### structlog aka logs that are not garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vE2QCYbpdps_"
   },
   "outputs": [],
   "source": [
    "import structlog\n",
    "log = structlog.get_logger()\n",
    "log.info(\"user_logged_in\", user_id=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q0_1KRugdvq3"
   },
   "source": [
    "### pyrsistent aka Immutable Data Structures That Actually Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6245,
     "status": "ok",
     "timestamp": 1753008812942,
     "user": {
      "displayName": "Its Me",
      "userId": "05939231591080462335"
     },
     "user_tz": -120
    },
    "id": "moaAR3ztd0NQ",
    "outputId": "f09d1040-fea9-44cd-d0d7-dedcc80507af"
   },
   "outputs": [],
   "source": [
    "!pip install pyrsistent\n",
    "from pyrsistent import pmap\n",
    "original = pmap({'a': 1})\n",
    "modified = original.set('b', 2)\n",
    "print(original)  # {'a': 1}\n",
    "print(modified)  # {'a': 1, 'b': 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLsZpMMjd_GX"
   },
   "source": [
    "### deepdiff aka Detect All the Tiny Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6963,
     "status": "ok",
     "timestamp": 1753008877582,
     "user": {
      "displayName": "Its Me",
      "userId": "05939231591080462335"
     },
     "user_tz": -120
    },
    "id": "LPEBcONpeDze",
    "outputId": "af5f5cbc-e6c7-4deb-c7bb-65a8d2db72b8"
   },
   "outputs": [],
   "source": [
    "!pip install DeepDiff\n",
    "from deepdiff import DeepDiff\n",
    "d1 = {\"a\": 1, \"b\": {\"x\": 10, \"y\": 20}}\n",
    "d2 = {\"a\": 1, \"b\": {\"x\": 15, \"y\": 20}}\n",
    "print(DeepDiff(d1, d2))\n",
    "# {'values_changed': {\"root['b']['x']\": {'new_value': 15, 'old_value': 10}}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUDgWkxyePTh"
   },
   "source": [
    "### anyio aka Async Done Right (And Sanely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFhdEAXJeUeJ"
   },
   "outputs": [],
   "source": [
    "import anyio\n",
    "async def do_work():\n",
    "    async with anyio.create_task_group() as tg:\n",
    "        tg.start_soon(some_async_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUgE-kuIechf"
   },
   "source": [
    "### tqdm.contrib.concurrent aka Multi-Threaded Progress Bars, Finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "4c9fd42f60da4d3db9e1015543d59ab1",
      "4a3396ae8cd0404694485300729010cf",
      "98d261e575764218be2c10b69a2a1265",
      "6c00b00522fa49718287082e7ffea858",
      "cc9789e23862449f8458d6c8c067e79f",
      "4c9daab1408941b4b5344f159b9507f5",
      "f37e228536864e749068e81870b232ac",
      "8717000f60ab4278b91e544825fb4917",
      "c58d68a5a5fc4d64b64c78bfc92f30b4",
      "afe0a0ca5f934cb4a8b2efd962072138",
      "21bcd16c9d994b64a277ec6f2ac6972e"
     ]
    },
    "executionInfo": {
     "elapsed": 7586,
     "status": "ok",
     "timestamp": 1753008988345,
     "user": {
      "displayName": "Its Me",
      "userId": "05939231591080462335"
     },
     "user_tz": -120
    },
    "id": "ZX_kUkJ3ehNw",
    "outputId": "aaa0a41f-3477-4095-dd71-ce4d9e726ac0"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.contrib.concurrent import thread_map\n",
    "def process(item):\n",
    "    # Simulate work\n",
    "    time.sleep(0.5)\n",
    "    return item * 2\n",
    "results = thread_map(process, range(100), max_workers=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuXdvx-9epog"
   },
   "source": [
    "### glom aka The Data Access Toolkit You Didn’t Know You Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5834,
     "status": "ok",
     "timestamp": 1753009040380,
     "user": {
      "displayName": "Its Me",
      "userId": "05939231591080462335"
     },
     "user_tz": -120
    },
    "id": "h1AVIzVuesuA",
    "outputId": "49c21f7a-dae3-49ca-f221-89644ee1bc76"
   },
   "outputs": [],
   "source": [
    "!pip install glom\n",
    "from glom import glom\n",
    "data = {\n",
    "    \"data\": {\n",
    "        \"items\": [{\"name\": \"Alpha\"}, {\"name\": \"Beta\"}]\n",
    "    }\n",
    "}\n",
    "print(glom(data, 'data.items.1.name'))  # Output: Beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "al2OAE3qe2sI"
   },
   "source": [
    "### diskcache aka When You Want Cache but Hate Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6157,
     "status": "ok",
     "timestamp": 1753009099339,
     "user": {
      "displayName": "Its Me",
      "userId": "05939231591080462335"
     },
     "user_tz": -120
    },
    "id": "6uP99aope5ag",
    "outputId": "1ddc9c67-1527-4199-c4fa-14ab6040833d"
   },
   "outputs": [],
   "source": [
    "!pip install diskcache\n",
    "import diskcache as dc\n",
    "cache = dc.Cache('./mycache')\n",
    "@cache.memoize()\n",
    "def expensive_calc(x):\n",
    "    print(\"Calculating...\")\n",
    "    return x * 42\n",
    "print(expensive_calc(10))  # Calculates and caches\n",
    "print(expensive_calc(10))  # Instant from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RnFifR2fFb4"
   },
   "source": [
    "### tenacity aka Retry Until It Works (Or You Rage Quit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "executionInfo": {
     "elapsed": 4099,
     "status": "error",
     "timestamp": 1753009153863,
     "user": {
      "displayName": "Its Me",
      "userId": "05939231591080462335"
     },
     "user_tz": -120
    },
    "id": "3PYpG9oxfIve",
    "outputId": "894cd7c1-b6ef-4b62-952d-7b365024fe56"
   },
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_fixed\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_fixed(2))\n",
    "def fragile_function():\n",
    "    print(\"Trying...\")\n",
    "    raise Exception(\"Oops\")\n",
    "fragile_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eghQI58fXHy"
   },
   "source": [
    "### boltons aka Batteries You Didn’t Know Were Missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 49,
     "status": "ok",
     "timestamp": 1753009215765,
     "user": {
      "displayName": "Its Me",
      "userId": "05939231591080462335"
     },
     "user_tz": -120
    },
    "id": "6v5j6WgPfakF",
    "outputId": "13ddfffe-63ff-4da3-bbc3-23f49f0353d9"
   },
   "outputs": [],
   "source": [
    "from boltons.iterutils import chunked\n",
    "for group in chunked(range(10), 3):\n",
    "    print(group)\n",
    "# Output: [0, 1, 2], [3, 4, 5], [6, 7, 8], [9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2lwI9iofjLz"
   },
   "source": [
    "## Python Libraries\n",
    "|Library|Description|\n",
    "|-------|-----------|\n",
    "|anyio|Async Done Right (And Sanely)|\n",
    "|boltons|Batteries You Didn’t Know Were Missing|\n",
    "|deepdiff|Detect All the Tiny Differences|\n",
    "|diskcache|When You Want Cache but Hate Complexity|\n",
    "|glom|The Data Access Toolkit You Didn’t Know You Needed|\n",
    "|gradio or streamlit|Rapid UI|\n",
    "|openai|access this non local llms|\n",
    "|pyinstrument|flamegraph-style profiler|\n",
    "|pyrsistent|Immutable Data Structures That Actually Work|\n",
    "|python-dotenv|Store API keys securely|\n",
    "|rich|For beautiful console logs|\n",
    "|structlog|Logs That Aren’t Garbage|\n",
    "|tenacity|Retry Until It Works (Or You Rage Quit)|\n",
    "|tqdm.contrib.concurrent|Multi-Threaded Progress Bars, Finally|\n",
    "|typer|Add a CLI interface to your automation scripts|\n",
    "|watchdog|Automatically run scripts when files change|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IvBugkn8gc75"
   },
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6XPpeTvgfQp"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"your_sk\"\n",
    "prompt = f\"\"\"\n",
    "You are an automation assistant trained in Python development and scripting.\n",
    "Your job is to take the following user input and perform a highly specific automated transformation.\n",
    "### Here's the task:\n",
    "{user_task}\n",
    "### Here's the input data (if any):\n",
    "{input_data}\n",
    "### Your job is to:\n",
    "- Write complete Python code for automating the task.\n",
    "- Include all necessary libraries and handle exceptions.\n",
    "- Optimize for readability and reuse.\n",
    "- Return only runnable Python code (no explanation).\n",
    "If required, use OpenAI's functions or APIs to complete the task.\n",
    "\"\"\"\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert Python automation engineer.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "code_snippet = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "print(code_snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6u5MGOOHglHo"
   },
   "source": [
    "### with UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mDtLh-MgngS"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "def auto_dev(task, data):\n",
    "    # Call GPT-4o with the automation prompt\n",
    "    return generate_code(task, data)  # Your GPT wrapper function\n",
    "gr.Interface(\n",
    "    fn=auto_dev,\n",
    "    inputs=[\"text\", \"text\"],\n",
    "    outputs=\"code\",\n",
    "    title=\"Auto Developer\",\n",
    "    description=\"Describe the task and paste input - get working Python code.\"\n",
    ").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcCPCC1Ag564"
   },
   "source": [
    "### Example of fixing broken CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4AXh0BCg7cH"
   },
   "outputs": [],
   "source": [
    "user_task = \"Fix malformed CSV where some rows have more or fewer columns than the header. Align data properly. Drop incomplete rows.\"\n",
    "input_data = \"path/to/broken.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQXcEB6ig_EY"
   },
   "source": [
    "### Generating tools (with 4 example tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QgHlve5hCoY"
   },
   "outputs": [],
   "source": [
    "dev_tasks = [\n",
    "    \"Convert JSON to CSV\",\n",
    "    \"Summarize Python logs\",\n",
    "    \"Extract SQL queries from text\",\n",
    "    \"Generate OpenAPI schemas from code comments\"\n",
    "]\n",
    "chain_prompt = f\"\"\"\n",
    "For each of the following tasks, create a ChatGPT prompt that:\n",
    "- Clearly defines the task\n",
    "- Requests Python code for automation\n",
    "- Optimizes for minimal dependencies\n",
    "- Returns only code, no commentary\n",
    "Tasks:\n",
    "{dev_tasks}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7tAwLfyhFpf"
   },
   "source": [
    "## Sources\n",
    "\n",
    "- https://blog.stackademic.com/this-chatgpt-prompt-unlocks-features-you-didnt-know-existed-fd175fc59d14\n",
    "- https://python.plainenglish.io/10-python-quality-of-life-libraries-i-wish-i-used-sooner-2aa5793b4a98\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOElOQ7y0Um5tMIa/jTDKwu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "21bcd16c9d994b64a277ec6f2ac6972e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4a3396ae8cd0404694485300729010cf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4c9daab1408941b4b5344f159b9507f5",
      "placeholder": "​",
      "style": "IPY_MODEL_f37e228536864e749068e81870b232ac",
      "value": "100%"
     }
    },
    "4c9daab1408941b4b5344f159b9507f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c9fd42f60da4d3db9e1015543d59ab1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4a3396ae8cd0404694485300729010cf",
       "IPY_MODEL_98d261e575764218be2c10b69a2a1265",
       "IPY_MODEL_6c00b00522fa49718287082e7ffea858"
      ],
      "layout": "IPY_MODEL_cc9789e23862449f8458d6c8c067e79f"
     }
    },
    "6c00b00522fa49718287082e7ffea858": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afe0a0ca5f934cb4a8b2efd962072138",
      "placeholder": "​",
      "style": "IPY_MODEL_21bcd16c9d994b64a277ec6f2ac6972e",
      "value": " 100/100 [00:07&lt;00:00, 13.97it/s]"
     }
    },
    "8717000f60ab4278b91e544825fb4917": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98d261e575764218be2c10b69a2a1265": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8717000f60ab4278b91e544825fb4917",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c58d68a5a5fc4d64b64c78bfc92f30b4",
      "value": 100
     }
    },
    "afe0a0ca5f934cb4a8b2efd962072138": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c58d68a5a5fc4d64b64c78bfc92f30b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc9789e23862449f8458d6c8c067e79f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f37e228536864e749068e81870b232ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
